{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a question-answering application over a corpus of private\n",
    "...\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Architecture](#architecture)\n",
    "1. [Creating the Inference Service](#creating-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and\n",
    "integrating the Vector Store ISVC with the LLM ISVC. ...\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. **Intercepting the User's Request**: The transformer acts as a gateway between the user and the LLM ISVC. When a user\n",
    "   sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. **Communicating with the Vector Store ISVC**: The transformer then takes the user's query and sends a POST request to the\n",
    "   Vector Store ISVC including the user's query in the payload, just like you did in the previous Notebook.\n",
    "1. **Receiving and Processing the Context**: The Vector Store ISVC responds by sending back the relevant context.\n",
    "1. **Combining the Context with the User's Query**: The transformer then combines the received context with the user's\n",
    "   original query using a prompt template. This creates an enriched prompt that contains both the user's original\n",
    "   question and the relevant context from our documents.\n",
    "1. **Forwarding the Enriched Query to the LLM Predictor**: Finally, the transformer forwards this enriched query to the LLM\n",
    "   predictor. The predictor then processes this query and generates a response, which is sent back to the transformer.\n",
    "   Steps 2 through 5 are transparent to the user.\n",
    "1. **Final response**:The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build one custom Docker image at this point for the transformer component. The\n",
    "source code and the Dockerfile is provided in the corresponding folder: `dockerfiles/transformer`.\n",
    "For your convenience, you can use the image we have pre-built for you: `harbor.ezml.local/dev/tritonserver:24.05-vllm-with-s3`\n",
    "\n",
    "Once ready, proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51ccb-f1d3-4132-a88b-62b84d90b11a",
   "metadata": {},
   "source": [
    "# Creating the Inference Service\n",
    "\n",
    "As before, you need to provide the name of the transofmer image You can leave any field empty to use the image we\n",
    "provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db302a3-e35f-4bb3-a2c5-24ee637c2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Model Inference Service</h2>\")\n",
    "display(heading)\n",
    "\n",
    "domain_input = widgets.Text(description='Domain:', placeholder=\"ezua1.local\")\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "name_input = widgets.Text(description='Inference Service Name:', placeholder=\"vllm-triton\")\n",
    "container_image_input = widgets.Text(description='Container Image:', placeholder=\"harbor.ezml.local/dev/tritonserver:24.05-vllm-with-s3\")\n",
    "mlflow_run_id_input = widgets.Text(description='Mlflow Run ID:', placeholder=\"d3682c0c64834c398ff5f0f0754cd255\")\n",
    "limit_cpu_input = widgets.Text(description='Limit CPU:', placeholder=\"4\")\n",
    "limit_memory_input = widgets.Text(description='Limit Memory:', placeholder=\"4Gi\")\n",
    "limit_gpu_input = widgets.Text(description='Limit GPU:', placeholder=\"1\")\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "domain = None\n",
    "mlflow_username = None\n",
    "mlflow_password = None\n",
    "\n",
    "def submit_button_clicked(b):    \n",
    "    os.environ[\"DOMAIN\"] = domain_input.value\n",
    "    os.environ[\"USERNAME\"] = username_input.value\n",
    "    os.environ[\"PASSWORD\"] = password_input.value\n",
    "    os.environ[\"NAME\"]= name_input.value\n",
    "    os.environ[\"CONTAINER_IMAGE\"] = mlflow_run_id_input.value\n",
    "    os.environ[\"MLFLOW_RUN_ID\"] = mlflow_run_id_input.value\n",
    "    os.environ[\"LIMIT_CPU\"] = limit_cpu_input.value\n",
    "    os.environ[\"LIMIT_MEMORY\"] = limit_memory_input.value\n",
    "    os.environ[\"LIMIT_GPU\"] = limit_gpu_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(domain_input, username_input, password_input, name_input,  container_image_input, mlflow_run_id_input, limit_cpu_input, limit_memory_input, limit_gpu_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545db2b-c5a1-4dfd-a382-3b4d964e9d79",
   "metadata": {},
   "source": [
    "Define and apply the LLM Inference Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"{os.environ[\"NAME\"]}\"\n",
    "spec:\n",
    "  predictor:\n",
    "    triton:\n",
    "      image: \"{os.environ[\"CONTAINER_IMAGE\"]}\"\n",
    "      env:\n",
    "      - name: DOMAIN\n",
    "        value: \"{os.environ[\"DOMAIN\"]}\"\n",
    "      - name: USERNAME\n",
    "        value: \"{os.environ[\"USERNAME\"]}\"\n",
    "      - name: PASSWORD\n",
    "        value: \"{os.environ[\"PASSWORD\"]}\"\n",
    "      - name: MLFLOW_RUN_ID\n",
    "        value: \"{os.environ[\"MLFLOW_RUN_ID\"]}\"\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: {os.environ[\"LIMIT_CPU\"]}\n",
    "          memory: \"{os.environ[\"LIMIT_MEMORY\"]}\"\n",
    "          nvidia.com/gpu: {os.environ[\"LIMIT_GPU\"]}\n",
    "        requests:\n",
    "          cpu: 1\n",
    "          memory: \"2Gi\"          \n",
    "      storageUri: pvc://kubeflow-shared-pvc/model_repository\n",
    "\"\"\"\n",
    "\n",
    "with open(\"vllm-triton.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vllm-triton.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built an LLM ISVC, and\n",
    "you've learned about the role of a transformer in enriching user queries with relevant context from our documents.\n",
    "Together with the Vector Store ISVC, these components form the backbone of your question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test the LLM ISVC, ensuring that it's working as\n",
    "expected and delivering accurate responses. This will help you gain confidence in your setup and prepare you for\n",
    "real-world applications. In the next Notebook, you invoke the LLM ISVC. You see how to construct suitable requests,\n",
    "communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}