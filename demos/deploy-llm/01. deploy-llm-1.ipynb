{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd468fc-763e-4d30-8747-93990276079e",
   "metadata": {},
   "source": [
    "# Deploy LLM for DEMO\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Download and Save a Model Locally](#Download-a-LLM-model-and-store-in-local-storage)\n",
    "2. [Set Up MLflow Credential and Environment values](#Set-Up-MLflow-Credential-and-Environment-values)\n",
    "3. [Logging the Downloaded Model as an Artifact](#Logging-the-Downloaded-Model-as-an-Artifact)\n",
    "4. [Register the model to MLflow Model registry](#Register-the-model-to-MLflow-Model-registry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94065c6-1324-41f7-aca8-0147e99462ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer \n",
    "import requests\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d55403-3a47-4588-9873-88b077159b77",
   "metadata": {},
   "source": [
    "# Download and Save a Model Locally\n",
    "\n",
    "Download a model from hugging face and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf32cd1-7d5f-4de8-a0f2-8fbc5b6eb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Model Name and Local Directory to Store</h2>\")\n",
    "display(heading)\n",
    "\n",
    "modelname_input = widgets.Text(description='Model name:', placeholder=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "modeldir_input = widgets.Text(description='Save to:', placeholder=\"./models/all-MiniLM-L6-v2\")\n",
    "\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "model_name = None\n",
    "model_dir = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global model_name, model_dir\n",
    "    model_name = modelname_input.value\n",
    "    model_dir = modeldir_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Configuration submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(modelname_input, modeldir_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a6bfd-a85b-4ec8-b0a0-5dde2e59bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"gpt2\"\n",
    "#model_dir = \"./models/\"+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502eadd-00c1-4ecf-817a-529980efcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the tokenizer, model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8698b-d411-4264-93ef-ff50ac2b4c09",
   "metadata": {},
   "source": [
    "# Set Up MLflow Credential and Environment values\n",
    "\n",
    "Set up MLflow for model tracking and register the model. Include authentication if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9bb9e-962a-4ed7-9cf2-84a0efc0791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>MLflow Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "domain_input = widgets.Text(description='Domain:', placeholder=\"ua.ezm.host\")\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "domain = None\n",
    "mlflow_username = None\n",
    "mlflow_password = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global domain, mlflow_username, mlflow_password\n",
    "    domain = domain_input.value\n",
    "    mlflow_username = username_input.value\n",
    "    mlflow_password = password_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(domain_input, username_input, password_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4288943-0805-47c8-a792-102311e48fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url = f\"https://keycloak.{domain}/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : mlflow_username,\n",
    "    \"password\" : mlflow_password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e8df4-9150-4c33-a960-5106436647fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ['MLFLOW_TRACKING_TOKEN']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow.mlflow.svc.cluster.local:5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25606b8-19a0-42bf-83b3-c09562f37326",
   "metadata": {},
   "source": [
    "# Logging the Downloaded Model as an Artifact\n",
    "\n",
    "To begin, you create a new experiment or utilize an existing one and log the model as an artifact of this\n",
    "experiment. Ultimately, you retrieve the URI that points to this artifact's location and provide it to the custom\n",
    "predictor component. By doing this, the custom predictor component understands how to fetch the artifact and serve it\n",
    "effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1654ebf-f83f-4c70-8da0-67ac57726dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc892254-69cf-43ae-94fe-58209a38afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('deploy-llm-demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c35ee-493b-4366-8419-4b58804dd462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Log the downloaded model as an artifact of the experiment\n",
    "class LLMModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts[\"model_dir\"])\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(context.artifacts[\"model_dir\"])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        inputs = self.tokenizer(model_input, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=model_name+\"_model\",\n",
    "        python_model=LLMModelWrapper(),\n",
    "        artifacts={\"model_dir\": model_dir},\n",
    "    )\n",
    "    uri_path = model_name+\"_model\"\n",
    "    \n",
    "    model_uri = f\"runs:/{run.info.run_id}/{uri_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8fd28-4275-430f-a3c2-6c875f0b7d8f",
   "metadata": {},
   "source": [
    "# Register the model to MLflow Model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc8f69-a04c-4cec-b6ce-86b7727e517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model registered with name: {model_name} and version: {result.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44b2a7-8a97-4daf-90b1-68915397566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "#sample_input = [\"This is a sample input for the gpt2 model.\"]\n",
    "#predicted_probs = loaded_model.predict(sample_input)\n",
    "#print(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e30d6a-511a-4f4a-9fc7-8a7f97db6ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
